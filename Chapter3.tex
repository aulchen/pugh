\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}


\newcommand{\B}{\mathbb{B}}
\newcommand{\osc}{\text{osc}}

\author{Arthur Chen}
\title{Chapter 3 Functions of a Real Variable}
\date{\today}

\begin{document}
\maketitle
\section*{Problem 1}
Assume that $f: \R \rightarrow \R$ satisfies $|f(t) - f(x)| \leq |t-x|^2$ for all $t, x$. Prove that $f$ is constant.

\begin{proof}
The assumption implies that for all $t, x$,
\[
0 \leq \left| \frac{f(t)-f(x)}{t-x} \right| = \frac{|f(t)-f(x)|}{|t-x|} \leq |t-x|
\]

implies that $f'(t) = \lim_{x \rightarrow t} \frac{f(t)-f(x)}{t-x} = 0$ at all $t$. The only functions with derivatives that are zero everywhere are constant functions.
\end{proof}

\section*{Problem 2}
A function $f: (a, b) \rightarrow \R$ satisfies a Holder condition of order $\alpha$ if $\alpha > 0$, and for some constant $H$ and all $u, x \in (a, b)$ se have

\[
|f(u) - f(x)| \leq H|u - x|^\alpha
\]

The function is said to be $\alpha$-Holder, with $\alpha$-Holder constant H.

\subsection*{Part a}
Prove that the $\alpha$-Holder function defined on $(a, b)$ is uniformly continuous and infer that it extends uniquely to a continuous function defined on $[a, b]$. Is the extended function $\alpha$-Holder?

\begin{proof}
Let $\epsilon > 0$ and define $\delta = (\frac{\epsilon}{H})^{1/\alpha}$. Then for all $u, x \in (a, b)$ such that $|u-x| < \delta$, we have
\[
|f(u)-f(x)| \leq H|u-x|^\alpha < \epsilon
\]
since $\alpha > 0$.
\end{proof}

By Problem 54 in Chapter 2, a uniformly continuous function defined on a metric space $S$ has a unique continuous extension on $\bar{S}$. Since $[a, b] = \bar{(a, b)}$, $f: (a, b) \rightarrow \R$ being uniformly continuous implies that $f$ extends uniquely to $g: [a, b] \rightarrow \R$, where $g$ is continuous. In fact, $g$ is uniformly continuous because it is continuous on a compact.

We claim that $g$ is $\alpha$-Holder on $[a, b]$. Let $x, y \in [a, b]$. If $x, y \in (a, b)$, this just follows because $g$ extends $f$.

Without loss of generality, let $x = a$ and let $y \in (a, b)$. Let $\epsilon > 0$ be fixed and arbitrary, and let $\delta>0$ be the corresponding continuity condition. Then

\[
|g(c) - g(a)| \leq |g(c) - g(a+\delta)| + |g(a) - g(a+\delta)|
\]

by the Triangle inequality. For the first term, because $c$ and $a+\delta$ are in the interval $(a, b)$, the Holder condition from $f$ extends to $g$, so

\[
|g(c) - g{f}(a+\delta)| \leq H|c-a-\delta|^\alpha \leq H|c-a|^\alpha
\]

because $\alpha > 0$ and $\delta > 0$. For the second term, continuity of $g$ means $|g(a) - g(a+\delta)| < \epsilon$. Thus

\[
|g(c) - g(a)| \leq H|c-a|^\alpha + \epsilon
\]

and $\epsilon$ can be made arbitrarily small. The case where $y = b$, and the case where $x=a$ and $y=b$ simultaneously, are essentially the same.

\subsection*{Part b}

What does $\alpha$-Holder continuity mean when $\alpha = 1$?

When $\alpha=1$, $\alpha$-Holder continuity simplifies to Lipschitz continuity.

\subsection*{Part c}

Prove that $\alpha$-Holder continuity when $\alpha > 1$ implies that $f$ is constant.

Let $x$ in the domain of $f$ be arbitrary. Dividing both sides by $|u-x|$,

\[
0 \leq \frac{|f(u)-f(x)|}{|u-x|} \leq H|u-x|^{\alpha-1}
\]

Let $u \rightarrow x$. Since $\alpha > 1$ the right side goes to $0$, implying $\frac{|f(u)-f(x)|}{|u-x|} \rightarrow 0$ and that $f'(x) = 0$ for all $x$ in $f$'s domain. The only functions with this property are constant functions.

\section*{Problem 3}

Assume that $f:(a, b) \rightarrow \R$ is differentiable.

\subsection*{Part a}

If $f'(x) > 0$ for all $x$, prove that $f$ is strictly monotone increasing.

\begin{proof}
Let $c, d \in (a, b), c < d$. Then because $f$ is differentiable on its domain, the Mean Value Theorem indicates that there is a point $\theta \in (c, d)$ such that

\[
f(c)-f(d) = f'(\theta)(d-c)
\]

Since $f'$ is always strictly positive and $c < d$, the right side is strictly positive.
\end{proof}

\subsection*{Part b}

If $f'(x) \geq 0$ for all $x$, what can you prove?

We can prove that $f$ is weakly monotone increasing. The proof is the same, except that $f'(\theta)(d-c)$ 
can be zero.

\section*{Problem 4}
Prove that $\sqrt{n+1} - \sqrt{n} \rightarrow 0$ as $n \rightarrow \infty$.

Consider the function $f(x) = \sqrt{x}$, and take a Taylor approximation of degree zero around $x = n$, where $n$ is a positive natural number. Then $P_0(x) = \sqrt{n}$. Use the Taylor approximation to approximate $x = n+1$. The Taylor remainder term is

\[
R(1) = \sqrt{n+1} - \sqrt{n}
\]

$\sqrt{x}$ is smooth when $x > 0$, and $n \geq 1$. Therefore, $f$ is smooth on $(n, n+1)$, and the Taylor approximation theorem states that there exists $\theta \in (n, n+1)$ such that

\[
R(1; n) = \sqrt{n+1} - \sqrt{n} = \frac{f'(\theta)}{1!}(1)^1 = \frac{1}{2}\theta^{-\frac{1}{2}}
\]

As $n \rightarrow \infty$, $\theta > n$ implies $\theta \rightarrow \infty$ implies $R(1; n) \rightarrow 0$ implies $\lim_{n \rightarrow \infty} \sqrt{n+1} - \sqrt{n} = 0$.

\section*{Problem 8}

\subsection*{Part b}

Find a formula for a continuous function defined on $[0, 1]$ that is differentiable on the interval $(0, 1)$, but not at the endpoints.

Consider the function

\[
f(x) = 
\begin{cases}
x\sin(\frac{1}{x}) & x \in (0, 1]\\
0 & \text{else}
\end{cases}
\]

$f$ is the composition of continuous functions on $(0, 1]$, so it is continuous on that interval. At $x=0$, we noting that for all $x \in (0, 1]$, we have

\[
-x \leq x \sin(\frac{1}{x}) \leq x
\]

implying that $\lim_{x \rightarrow 0^+} f(x) = 0 = f(0)$ by the Squeeze theorem. This implies that $f(x)$ is continuous at $x=0$, and thus $[0, 1]$. $\frac{1}{x}$ is differentiable on $\R - {0}$, so $f(x)$ is differentiable on $(0, 1]$.

Taking the definition of derivative to attempt to evaluate $f'(0)$,

\[
f'(0) = \lim_{x \rightarrow 0^+} \frac{f(x) - f(0)}{x - 0} = \lim_{x \rightarrow 0^+} \sin(\frac{1}{x})
\]

which does not exist. Thus $f(x)$ is differentiable on $(0, 1]$.

Consider the function

\[
g(x) = f(x) + f(1-x)
\]

This consists of $f$ and $f$ reflected about the line $x = \frac{1}{2}$ added together. From the above, $g$ is continuous on $[0, 1]$, and differentiable on $(0, 1)$, but not $0$ or $1$.

\subsection*{Part c}

Does the Mean Value Theorem apply to such a function?

Yes, since the Mean Value Theorem only requires the function to be differentiable on the open interval. In this case, the Mean Value Theorem states there is a point $\theta \in (0, 1)$ such that $g'(\theta) = 0$. We can probably prove that a point exists by using the Intermediate Value Theorem on $g'(x)$ since it's continuous on $(0, 1)$, but I'm too lazy at the moment.

\section*{Problem 10}

Concoct a function $f: \R \rightarrow \R$ with a discontinuity of the second kind at $x = 0$ such that $f$ does not have the intermediate value property there. Infer that it is incorrect to assert that functions without jumps are Darboux continuous.

Consider the function
\[
f(x) = 
\begin{cases}
x & x \in \R - \Q \\
1 & \text{else}
\end{cases}
\]

$f$ is continuous at $x=1$ and discontinuous everywhere else. These discontinuities are discontinuities of the second kind, since left and right limits don't exist when $x$ is not $1$. $f(x)$ clearly does not have the intermediate value property, as except for $1$, $f$ assumes no rational values. Since this is a function without jump discontinuities but does not possess the intermediate value property, functions without jumps are not necessarily Darboux continuous.

\section*{Problem 11}

Let $f: (a, b) \rightarrow \R$ be given.

\subsection*{Part a}

If $f''(x)$ exists, prove that

\[
\lim_{h \rightarrow 0} \frac{f(x-h) - 2f(x) + f(x+h)}{h^2} = f''(x)
\]

Denote $F(x) = \lim_{h \rightarrow 0} \frac{f(x-h) - 2f(x) + f(x+h)}{h^2}$. Since $f$ is twice differentiable, we take take a second-order Taylor expansion of $f$ around $x$, getting

\[
f(x+h) = f(x) + hf'(x) + \frac{1}{2} h^2 f''(x) + R(x)
\]

where $R(x)$ is second-order flat at $h = 0$, i.e. $\lim_{h \rightarrow 0} R(x)/h^2 = 0$. Similarly,

\[
f(x-h) = f(x) - hf'(x) + \frac{1}{2} h^2 f''(x) + S(x)
\]

where $S(x)$ is second-order flat at $h = 0$. Substituting,

\[
F(x) =
\lim_{h \rightarrow 0} \frac{h^2 f''(x) + R(x) + S(x)}{h^2}
= f''(x)
\]

since the $f(x)$ and $hf'(x)$ terms cancel, and $R(x)$ and $S(x)$ are second-order flat.

\subsection*{Part b}

Find an example that this limit can exist even when $f''(x)$ fails to exist.

Let $f(x) = x|x|$. Taking the first derivative, when $x>0$, $f(x) = x^2$, so $f'(x) = 2x$. Similarly, when $x<0$, $f'(x) = -2x$. When $x=0$,

\[
f'(0) = \lim_{h \rightarrow 0} \frac{f(0+h) - f(0)}{h} = \lim_{h \rightarrow 0} \frac{h|h|}{h} = \lim_{h \rightarrow 0} |h| = 0
\]

Thus

\[
f'(x) =
\begin{cases}
2x & x \geq 0 \\
-2x & x < 0
\end{cases}
\]

As previously stated, $f''(0)$ does not exist, since

\[
f''(0) = \lim_{h \rightarrow 0} \frac{f'(x+h) - f'(x)}{h} = \lim_{h \rightarrow 0} \frac{f'(h)}{h}
\]

which does not exist, since the limit from the positive direction is $2$ and the limit from the negative direction is $-2$.

Despite this, the partial difference approximation exists at $x = 0$. The partial difference approximation from the right is

\[
\lim_{h \rightarrow 0^+} \frac{f(-h) + f(h)}{h^2} = 
\lim_{h \rightarrow 0^+} \frac{-h|-h| + h|h|}{h^2} =
\lim_{h \rightarrow 0^+} \frac{0}{h^2} = \infty
\]

Similarly,

\[
\lim_{h \rightarrow 0^-} \frac{f(-h) + f(h)}{h^2} = 
\lim_{h \rightarrow 0^-} \frac{h|h| + -h|-h|}{h^2} =
\lim_{h \rightarrow 0^-} \frac{0}{h^2} = \infty
\]

Thus the difference approximation exists at $x=0$, even though $f''(0)$ does not exist.

\section*{Problem 15}

Define $f(x) = x^2$ if $x < 0$ and $f(x) = x + x^2$ if $x \geq 0$. Differentiation gives $f''(x) = 2$. This is bogus. Why?

By the Fundamental Theorem of Calculus, if $G$ is an antiderivative of $g$, then $g$ equals the derivative of $G$ where $g$ is continuous. In this case, the standard power rule only applies when $x \neq 0$, since there is a discontinuity there.

Specifically, we have $f''(0)$ does not exist, since $f'(x) = 2x$ when $x \geq 0$, and $f'(x) = 2x + 1$ when $x < 0$. $f'(x)$ is discontinuous at $x=0$, so its derivative does not exist there.

\section*{Problem 16}

$\log x$ is defined to be $\int_1^x 1/t dt$ for $x > 0$. Using only the mathematics explained in this chapter,

\subsection*{Part a}

Prove that $\log$ is a smooth function.

By the Fundamental Theorem of Calculus, the indefinite integral of a Riemann integrable function is continuous with respect to $x$. Thus, $\log x$ is continuous. Its derivative, again by the Fundamental Theorem of Calculus, is $\frac{d}{dx} \int_1^x 1/t dx = 1/x$ when $x > 0$, which is continuous. $1/x$ itself is smooth, so it has derivatives of all orders, which are continuous. Thus $\log x$ is smooth.

\subsection*{Part b}

Prove that $\log(xy) = \log x + \log y$ for all $x, y > 0$.

For any given $y > 0$, define $f(x) = \log xy - \log x - \log y$. By definition,

\begin{align*}
f(x) &= \int_1^{xy} 1/t dt - \int_1^x 1/t dt - \int_1^y 1/t dt \\
&= \int_x^{xy} 1/t dt - \int_1^y 1/t dt
\end{align*}

When $x = 0$, $f(x) = \int_1^{y} 1/t dt - \int_1^{y} 1/t dt = 0$.

We now evaluate $f'(x)$. Splitting the integrals, for all $x>0$, we can find a constant $0 < c < x$. Then

\[
f(x) = \int_c^{xy} 1/t dt - \int_c^x 1/t dt - \int_1^y 1/t dt
\]

By the Fundamental Theorem of Calculus, $\frac{d}{dx} \int_c^x 1/t dt = 1/x$ since $1/t$ is continuous on $[c, \infty)$. By the Chain Rule, $\frac{d}{dx} \int_c^{xy} 1/t dt = y\frac{1}{xy} = 1/x$. Thus, $f'(x) = 0$ for all $x > 0$. $\int_1^y 1/t dt$ is constant with regards to $x$, and thus has derivative zero. The only functions with derivatives equal to zero everywhere are constant functions, and since $f(1) = 0$, this implies that $f(x) = 0$. Thus $\log xy = \log x + \log y$.

\subsection*{Part c}

Prove that $\log$ is strictly monotone increasing and its range is all of $\R$.

$\frac{d}{dx} \log x = 1/x$, which is strictly positive for all $x > 0$. Thus $\log x$ is strictly monotone increasing.

TO FINISH.

\section*{Problem 29}

Prove that the interval $[a, b]$ is not a zero set.

\subsection*{Part a}

Explain why the following observation is not a solution to the problem: "Every open interval that contains $[a, b]$ has length $> b-a$."

This 'solution' does not consider the possibility that there is a union of open sets that cover $[a, b]$ such that their sum of their lengths can be made arbitrarily small.

\subsection*{Part b}

Instead, suppose there is a "bad" covering of $[a, b]$ by open intervals $\{I_i\}$ whose total length is $< b-a$, and justify the following steps in the proof by contradiction.

I will define a good covering as a covering of $[a, b]$ by open intervals $\{J\}$ such that the total length of the intervals in $\{J\}$ is greater than or equal to $b-a$.

\subsubsection*{i}

It is enough to deal with finite bad coverings.

Let $\{I\}$ be an infinite bad covering of $[a, b]$. Because $\{I\}$ is an open cover of compact $[a, b]$, it reduces to a finite subcovering $\{I_i\}$. Thus, either $\{I\}$ reduces to a finite bad covering, or it reduces to a good covering. If $\{I\}$ reduces to a good covering $\{J_i\}$, then  $\{J_i\} \subset \{I\}$ and the sum of the intervals in $\{J_i\}$ being $\geq b-a$ implies that the sum of the intervals in $\{I\}$ is $\geq b-a$. Thus $\{I\}$ is an infinite good covering, which contradicts the assumption that $\{I\}$ is a bad covering.

Thus, if $\{I\}$ is an infinite bad covering, it reduces to a finite bad covering. Contrapositively, if there are no finite bad coverings, then there are no infinite bad coverings, and the theorem is proven.

\subsubsection*{ii}

Let $\B = \{I_1, \dots I_n\}$ be a bad covering such that $n$ is minimal among all bad coverings.

There is at least one finite bad covering, by assumption. $n=1$ is a lower bound for the size of bad coverings. Then because $\R$ is complete, there exists a greatest lower bound for the sizes of the bad coverings, denoted $c$.

The must be a finite bad covering $\{C\}$ such that the size of $|\{C\}| = c$. Suppose not. Then all bad coverings have size $> c$, and size the sizes of the bad coverings must be integers, all bad coverings have size $\geq c+1$. This contradicts the assumption that $c$ is a greatest lower bound. This bad covering $\{C\}$ is the bad covering with minimal $n$ among all bad coverings.

\subsubsection*{iii}

Show that no bad covering has $n=1$ so we have $n \geq 2$.

This follows from the observation in Part a.

\subsubsection*{iv}

Show that it is no loss of generality to assume $a \in I_1$ and $I_1 \cap I_2 \neq \emptyset$.

There exists at least one interval such that $a \in I_j$, and we are free to denote that interval $I_1$.

There must exist an interval that intersects $I_1$. Suppose not. Let $d_1$ be the right endpoint of $I_1$, and let $c_2, c_3 \dots c_n$ be the left endpoints of the other intervals in the bad covering, and let $c = \min\{c_1 \dots c_n\}$. Then $\frac{c-d}{2}$ is not covered by the bad covering, contradicting the assumption that $\{I\}$ is a covering. Thus, there exists an interval in $\{I\}$ that intersects $I_1$. Denote it $I_2$. By construction, $I_1 \cap I_2$ is nonempty.

\subsubsection*{v}

Show that $I = I_1 \cup I_2$ is an open interval and $|I| < |I_1| + |I_2|$.

If $I_1 \subset I_2$ or $I_2 \subset I_1$, $I_1 \cup I_2$ is trivially an open interval. Otherwise, $I_1 \cup I_2$ is the open because it is the union of open sets, connected because it is the union of two connected sets with a common point, and bounded because it is the finite union of bounded sets. Therefore $I_1 \cup I_2$ is a open, connected, and bounded subset of $\R$, and by the theorems shown in Chapter 2 Problem 31, open, connected, and bounded subsets of $\R$ are open intervals.

\begin{lemma}
Let $C, D \subset \R$ be (bounded) intervals that intersect, and let $E = C + D$. Then $|E| < |C| + |D|$.

\begin{proof}
If $C$ is a subset of $D$ or vice versa, the proof is trivial. Without loss of generality, let the left endpoint of $C$ be less than the left endpoint of $D$. Denote $c$ as the right endpoint of $C$, and $d$ the left endpoint of $D$. $d < c$, otherwise the two intervals do not intersect. Letting $\epsilon = c - d > 0$, the total length of $E$ is $|C| + |D| - \epsilon$, which is strictly less than $|C| + |D|$.
\end{proof}
\end{lemma}

By using the above Lemma, we see that $|I| < |I_1| + |I_2|$.

\subsubsection*{vi}

Show that $\B' = \{I, I_3, \dots I_n\}$ is a bad covering of $[a, b]$ with fewer intervals, contradicting the minimality of $n$.

Let $x \in [a, b]$. Since $\B$ is a covering of $[a, b]$, there exists $i \in 1, 2 \dots n$ such that $x \in I_i$. If $i \geq 3$, then because $I_i \in \B'$, $x$ is also covered by $\B'$. If $i = 1, 2$, then $x \in I = I_1 \cup I_2$, so $x$ is still covered by $\B'$. $\B'$ is a covering by open intervals, because $I$ is an open interval. $\B'$ is a bad covering. $|I| < |I_1| + |I_2|$ implies that $|I| + \sum_{j=3}^n I_j < \sum_{i=1}^n I_i < b-a$, implying that the total length of $\B'$ is less than the total length of $\B$. Thus $\B'$ is a bad covering with fewer intervals than $\B$, contradicting the assumption that $\B$ is the minimal bad covering. Thus, there are no bad coverings of $[a, b]$, coverings of $[a, b]$ can not have arbitrarily small length, and $[a, b]$ is not a zero set.

\section*{Problem 34}

Assume that $\psi: [a, b] \rightarrow \R$ is continuously differentiable. A critical point of $\psi$ is an $x$ such that $\psi'(x) = 0$. A critical value is a number $y$ such that for at least one critical point $x$ we have $y = \psi(x)$.

\subsection*{Part a}

Prove that the set of critical values is a zero set. (This is the Morse-Sard Theorem in dimension one.)

We first divide critical points into two types. I shall define a \textbf{critical point of type 1} of a function $f$ as a critical point $x$ such that $f(x)$ is uniformly zero in an open neighborhood of $x$. In other words, there exists a $\epsilon > 0$ such that for all $y$ such that $|x-y| < \epsilon$, $f(y) = 0$. I will denote a \textbf{critical point of type 2} as all other critical points. It's clear that the disjoint union of these two types is the set of all critical points of $f$. I will define \textbf{critical values of type 1} to be critical values that correspond to critical points of type 1, and \textbf{critical values of type 2} similarly. It's clear that the disjoint union of these sets is the set of all critical values of $f$. I have no idea if this characterization is standard, but that's what I've come up with.

\begin{corollary}
Let $x$ be a critical point of type 1 for $f$. Then $f$ is continuous at $x$.
\begin{proof}
Near $x$, $f$ is a constant zero function, and constant functions are continuous.
\end{proof}
\end{corollary}

The immediate characterization for critical points of type 2 is stated below.

\begin{corollary}
Let $x$ be a critical point of type 2 of $f$. Then for all $\epsilon > 0$, there exists $y \in (x-\epsilon, x + \epsilon)$ such that $f(y) \neq 0$.
\begin{proof}
If this is not true, then $x$ is a critical point of type 1.
\end{proof} 
\end{corollary} 

We start with a lemma on critical points of type 1.

\begin{lemma}
Let $x$ be a critical point of type 1 for $\psi'(x)$. Then on the neighborhood where $\psi'(x) = 0$, there is only one critical value. Specifically, if $\psi'(x) = 0$ on an interval $(c, d) \subset [a, b]$, then $\psi(c)$ is the only critical value on that interval.
\begin{proof}
By the Fundamental Theorem of Calculus, for $x \in [c, d]$, $\psi(x) = \psi(c) + \int_c^x \psi'(x) dx = \psi(c)$ since $\psi'(x) = 0$ on the interval. 
\end{proof}
\end{lemma}

We now want to prove the main theorem for critical points of type 2.

\begin{theorem}
Let $f$ be continuous on $[a, b]$. Then $f$ has finite critical points of type 2. (Or maybe it's countable? I just need to show it's not uncountable.)
\begin{proof}
\textbf{TODO}. Something about oscillations or something.
\end{proof}
\end{theorem}

\begin{corollary}
Let $f$ be continuous on $[a, b]$. Then $f$ has finite critical values of type 2.
\begin{proof}
Immediate consequence of the above theorem.
\end{proof}
\end{corollary}

We now want to prove the main theorem for critical values corresponding to critical points of type 1.

\begin{theorem}
If $f: [a, b] \rightarrow \R$, then $f$ has countably many critical values of type 1.
\begin{proof}
\textbf{TODO}. Some sort of 1/k interval counting trick, since the critical points of type 1 can be grouped up into disjoint open intervals, except for some endpoint behavior. Then the countable intervals with critical points of type 1 map to at most countable critical values, using the lemma thingy with the fundamental theorem of calculus further up the page. Need to iron out the kinks.
\end{proof}
\end{theorem}

\begin{theorem}
The critical values of $f$ form a zero set.
\begin{proof}
The union of a countable set with a finite set is a countable set, which is a zero set.
\end{proof}
\end{theorem}

\end{document}