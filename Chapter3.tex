\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\B}{\mathbb{B}}
\newcommand{\osc}{\text{osc}}
\newcommand{\diam}{\text{diam}}

\author{Arthur Chen}
\title{Chapter 3 Functions of a Real Variable}
\date{\today}

\begin{document}
\maketitle
\section*{Problem 1}
Assume that $f: \R \rightarrow \R$ satisfies $|f(t) - f(x)| \leq |t-x|^2$ for all $t, x$. Prove that $f$ is constant.

\begin{proof}
The assumption implies that for all $t, x$,
\[
0 \leq \left| \frac{f(t)-f(x)}{t-x} \right| = \frac{|f(t)-f(x)|}{|t-x|} \leq |t-x|
\]

implies that $f'(t) = \lim_{x \rightarrow t} \frac{f(t)-f(x)}{t-x} = 0$ at all $t$. The only functions with derivatives that are zero everywhere are constant functions.
\end{proof}

\section*{Problem 2}
A function $f: (a, b) \rightarrow \R$ satisfies a Holder condition of order $\alpha$ if $\alpha > 0$, and for some constant $H$ and all $u, x \in (a, b)$ se have

\[
|f(u) - f(x)| \leq H|u - x|^\alpha
\]

The function is said to be $\alpha$-Holder, with $\alpha$-Holder constant H.

\subsection*{Part a}
Prove that the $\alpha$-Holder function defined on $(a, b)$ is uniformly continuous and infer that it extends uniquely to a continuous function defined on $[a, b]$. Is the extended function $\alpha$-Holder?

\begin{proof}
Let $\epsilon > 0$ and define $\delta = (\frac{\epsilon}{H})^{1/\alpha}$. Then for all $u, x \in (a, b)$ such that $|u-x| < \delta$, we have
\[
|f(u)-f(x)| \leq H|u-x|^\alpha < \epsilon
\]
since $\alpha > 0$.
\end{proof}

By Problem 54 in Chapter 2, a uniformly continuous function defined on a metric space $S$ has a unique continuous extension on $\bar{S}$. Since $[a, b] = \bar{(a, b)}$, $f: (a, b) \rightarrow \R$ being uniformly continuous implies that $f$ extends uniquely to $g: [a, b] \rightarrow \R$, where $g$ is continuous. In fact, $g$ is uniformly continuous because it is continuous on a compact.

We claim that $g$ is $\alpha$-Holder on $[a, b]$. Let $x, y \in [a, b]$. If $x, y \in (a, b)$, this just follows because $g$ extends $f$.

Without loss of generality, let $x = a$ and let $y \in (a, b)$. Let $\epsilon > 0$ be fixed and arbitrary, and let $\delta>0$ be the corresponding continuity condition. Then

\[
|g(c) - g(a)| \leq |g(c) - g(a+\delta)| + |g(a) - g(a+\delta)|
\]

by the Triangle inequality. For the first term, because $c$ and $a+\delta$ are in the interval $(a, b)$, the Holder condition from $f$ extends to $g$, so

\[
|g(c) - g{f}(a+\delta)| \leq H|c-a-\delta|^\alpha \leq H|c-a|^\alpha
\]

because $\alpha > 0$ and $\delta > 0$. For the second term, continuity of $g$ means $|g(a) - g(a+\delta)| < \epsilon$. Thus

\[
|g(c) - g(a)| \leq H|c-a|^\alpha + \epsilon
\]

and $\epsilon$ can be made arbitrarily small. The case where $y = b$, and the case where $x=a$ and $y=b$ simultaneously, are essentially the same.

\subsection*{Part b}

What does $\alpha$-Holder continuity mean when $\alpha = 1$?

When $\alpha=1$, $\alpha$-Holder continuity simplifies to Lipschitz continuity.

\subsection*{Part c}

Prove that $\alpha$-Holder continuity when $\alpha > 1$ implies that $f$ is constant.

Let $x$ in the domain of $f$ be arbitrary. Dividing both sides by $|u-x|$,

\[
0 \leq \frac{|f(u)-f(x)|}{|u-x|} \leq H|u-x|^{\alpha-1}
\]

Let $u \rightarrow x$. Since $\alpha > 1$ the right side goes to $0$, implying $\frac{|f(u)-f(x)|}{|u-x|} \rightarrow 0$ and that $f'(x) = 0$ for all $x$ in $f$'s domain. The only functions with this property are constant functions.

\section*{Problem 3}

Assume that $f:(a, b) \rightarrow \R$ is differentiable.

\subsection*{Part a}

If $f'(x) > 0$ for all $x$, prove that $f$ is strictly monotone increasing.

\begin{proof}
Let $c, d \in (a, b), c < d$. Then because $f$ is differentiable on its domain, the Mean Value Theorem indicates that there is a point $\theta \in (c, d)$ such that

\[
f(c)-f(d) = f'(\theta)(d-c)
\]

Since $f'$ is always strictly positive and $c < d$, the right side is strictly positive.
\end{proof}

\subsection*{Part b}

If $f'(x) \geq 0$ for all $x$, what can you prove?

We can prove that $f$ is weakly monotone increasing. The proof is the same, except that $f'(\theta)(d-c)$ 
can be zero.

\section*{Problem 4}
Prove that $\sqrt{n+1} - \sqrt{n} \rightarrow 0$ as $n \rightarrow \infty$.

Consider the function $f(x) = \sqrt{x}$, and take a Taylor approximation of degree zero around $x = n$, where $n$ is a positive natural number. Then $P_0(x) = \sqrt{n}$. Use the Taylor approximation to approximate $x = n+1$. The Taylor remainder term is

\[
R(1) = \sqrt{n+1} - \sqrt{n}
\]

$\sqrt{x}$ is smooth when $x > 0$, and $n \geq 1$. Therefore, $f$ is smooth on $(n, n+1)$, and the Taylor approximation theorem states that there exists $\theta \in (n, n+1)$ such that

\[
R(1; n) = \sqrt{n+1} - \sqrt{n} = \frac{f'(\theta)}{1!}(1)^1 = \frac{1}{2}\theta^{-\frac{1}{2}}
\]

As $n \rightarrow \infty$, $\theta > n$ implies $\theta \rightarrow \infty$ implies $R(1; n) \rightarrow 0$ implies $\lim_{n \rightarrow \infty} \sqrt{n+1} - \sqrt{n} = 0$.

\section*{Problem 8}

\subsection*{Part b}

Find a formula for a continuous function defined on $[0, 1]$ that is differentiable on the interval $(0, 1)$, but not at the endpoints.

Consider the function

\[
f(x) = 
\begin{cases}
x\sin(\frac{1}{x}) & x \in (0, 1]\\
0 & \text{else}
\end{cases}
\]

$f$ is the composition of continuous functions on $(0, 1]$, so it is continuous on that interval. At $x=0$, we noting that for all $x \in (0, 1]$, we have

\[
-x \leq x \sin(\frac{1}{x}) \leq x
\]

implying that $\lim_{x \rightarrow 0^+} f(x) = 0 = f(0)$ by the Squeeze theorem. This implies that $f(x)$ is continuous at $x=0$, and thus $[0, 1]$. $\frac{1}{x}$ is differentiable on $\R - {0}$, so $f(x)$ is differentiable on $(0, 1]$.

Taking the definition of derivative to attempt to evaluate $f'(0)$,

\[
f'(0) = \lim_{x \rightarrow 0^+} \frac{f(x) - f(0)}{x - 0} = \lim_{x \rightarrow 0^+} \sin(\frac{1}{x})
\]

which does not exist. Thus $f(x)$ is differentiable on $(0, 1]$.

Consider the function

\[
g(x) = f(x) + f(1-x)
\]

This consists of $f$ and $f$ reflected about the line $x = \frac{1}{2}$ added together. From the above, $g$ is continuous on $[0, 1]$, and differentiable on $(0, 1)$, but not $0$ or $1$.

\subsection*{Part c}

Does the Mean Value Theorem apply to such a function?

Yes, since the Mean Value Theorem only requires the function to be differentiable on the open interval. In this case, the Mean Value Theorem states there is a point $\theta \in (0, 1)$ such that $g'(\theta) = 0$. We can probably prove that a point exists by using the Intermediate Value Theorem on $g'(x)$ since it's continuous on $(0, 1)$, but I'm too lazy at the moment.

\section*{Problem 10}

Concoct a function $f: \R \rightarrow \R$ with a discontinuity of the second kind at $x = 0$ such that $f$ does not have the intermediate value property there. Infer that it is incorrect to assert that functions without jumps are Darboux continuous.

Consider the function
\[
f(x) = 
\begin{cases}
x & x \in \R - \Q \\
1 & \text{else}
\end{cases}
\]

$f$ is continuous at $x=1$ and discontinuous everywhere else. These discontinuities are discontinuities of the second kind, since left and right limits don't exist when $x$ is not $1$. $f(x)$ clearly does not have the intermediate value property, as except for $1$, $f$ assumes no rational values. Since this is a function without jump discontinuities but does not possess the intermediate value property, functions without jumps are not necessarily Darboux continuous.

\section*{Problem 11}

Let $f: (a, b) \rightarrow \R$ be given.

\subsection*{Part a}

If $f''(x)$ exists, prove that

\[
\lim_{h \rightarrow 0} \frac{f(x-h) - 2f(x) + f(x+h)}{h^2} = f''(x)
\]

Denote $F(x) = \lim_{h \rightarrow 0} \frac{f(x-h) - 2f(x) + f(x+h)}{h^2}$. Since $f$ is twice differentiable, we take take a second-order Taylor expansion of $f$ around $x$, getting

\[
f(x+h) = f(x) + hf'(x) + \frac{1}{2} h^2 f''(x) + R(x)
\]

where $R(x)$ is second-order flat at $h = 0$, i.e. $\lim_{h \rightarrow 0} R(x)/h^2 = 0$. Similarly,

\[
f(x-h) = f(x) - hf'(x) + \frac{1}{2} h^2 f''(x) + S(x)
\]

where $S(x)$ is second-order flat at $h = 0$. Substituting,

\[
F(x) =
\lim_{h \rightarrow 0} \frac{h^2 f''(x) + R(x) + S(x)}{h^2}
= f''(x)
\]

since the $f(x)$ and $hf'(x)$ terms cancel, and $R(x)$ and $S(x)$ are second-order flat.

\subsection*{Part b}

Find an example that this limit can exist even when $f''(x)$ fails to exist.

Let $f(x) = x|x|$. Taking the first derivative, when $x>0$, $f(x) = x^2$, so $f'(x) = 2x$. Similarly, when $x<0$, $f'(x) = -2x$. When $x=0$,

\[
f'(0) = \lim_{h \rightarrow 0} \frac{f(0+h) - f(0)}{h} = \lim_{h \rightarrow 0} \frac{h|h|}{h} = \lim_{h \rightarrow 0} |h| = 0
\]

Thus

\[
f'(x) =
\begin{cases}
2x & x \geq 0 \\
-2x & x < 0
\end{cases}
\]

As previously stated, $f''(0)$ does not exist, since

\[
f''(0) = \lim_{h \rightarrow 0} \frac{f'(x+h) - f'(x)}{h} = \lim_{h \rightarrow 0} \frac{f'(h)}{h}
\]

which does not exist, since the limit from the positive direction is $2$ and the limit from the negative direction is $-2$.

Despite this, the partial difference approximation exists at $x = 0$. The partial difference approximation from the right is

\[
\lim_{h \rightarrow 0^+} \frac{f(-h) + f(h)}{h^2} = 
\lim_{h \rightarrow 0^+} \frac{-h|-h| + h|h|}{h^2} =
\lim_{h \rightarrow 0^+} \frac{0}{h^2} = \infty
\]

Similarly,

\[
\lim_{h \rightarrow 0^-} \frac{f(-h) + f(h)}{h^2} = 
\lim_{h \rightarrow 0^-} \frac{h|h| + -h|-h|}{h^2} =
\lim_{h \rightarrow 0^-} \frac{0}{h^2} = \infty
\]

Thus the difference approximation exists at $x=0$, even though $f''(0)$ does not exist.

\section*{Problem 15}

Define $f(x) = x^2$ if $x < 0$ and $f(x) = x + x^2$ if $x \geq 0$. Differentiation gives $f''(x) = 2$. This is bogus. Why?

By the Fundamental Theorem of Calculus, if $G$ is an antiderivative of $g$, then $g$ equals the derivative of $G$ where $g$ is continuous. In this case, the standard power rule only applies when $x \neq 0$, since there is a discontinuity there.

Specifically, we have $f''(0)$ does not exist, since $f'(x) = 2x$ when $x \geq 0$, and $f'(x) = 2x + 1$ when $x < 0$. $f'(x)$ is discontinuous at $x=0$, so its derivative does not exist there.

\section*{Problem 16}

$\log x$ is defined to be $\int_1^x 1/t dt$ for $x > 0$. Using only the mathematics explained in this chapter,

\subsection*{Part a}

Prove that $\log$ is a smooth function.

By the Fundamental Theorem of Calculus, the indefinite integral of a Riemann integrable function is continuous with respect to $x$. Thus, $\log x$ is continuous. Its derivative, again by the Fundamental Theorem of Calculus, is $\frac{d}{dx} \int_1^x 1/t dx = 1/x$ when $x > 0$, which is continuous. $1/x$ itself is smooth, so it has derivatives of all orders, which are continuous. Thus $\log x$ is smooth.

\subsection*{Part b}

Prove that $\log(xy) = \log x + \log y$ for all $x, y > 0$.

For any given $y > 0$, define $f(x) = \log xy - \log x - \log y$. By definition,

\begin{align*}
f(x) &= \int_1^{xy} 1/t dt - \int_1^x 1/t dt - \int_1^y 1/t dt \\
&= \int_x^{xy} 1/t dt - \int_1^y 1/t dt
\end{align*}

When $x = 1$, $f(x) = \int_1^{y} 1/t dt - \int_1^{y} 1/t dt = 0$.

We now evaluate $f'(x)$. Splitting the integrals, for all $x>0$, we can find a constant $0 < c < x$. Then

\[
f(x) = \int_c^{xy} 1/t dt - \int_c^x 1/t dt - \int_1^y 1/t dt
\]

By the Fundamental Theorem of Calculus, $\frac{d}{dx} \int_c^x 1/t dt = 1/x$ since $1/t$ is continuous on $[c, \infty)$. By the Chain Rule, $\frac{d}{dx} \int_c^{xy} 1/t dt = y\frac{1}{xy} = 1/x$. $\int_1^y 1/t dt$ is constant with regards to $x$, and thus has derivative zero. Thus, $f'(x) = 0$ for all $x > 0$. The only functions with derivatives equal to zero everywhere are constant functions, and since $f(1) = 0$, this implies that $f(x) = 0$. Thus $\log xy = \log x + \log y$.

\subsection*{Part c}

Prove that $\log$ is strictly monotone increasing and its range is all of $\R$.

$\frac{d}{dx} \log x = 1/x$, which is strictly positive for all $x > 0$. Thus $\log x$ is strictly monotone increasing.

We know that $\log (1) = 0$. Going to the right, let $a_k = \frac{1}{k}$. Because $\frac{1}{t}$ is decreasing, for all $t \in [k, k+1]$, $\frac{1}{t} \leq a_{k+1}$. Thus because $\sum_{k=2}^\infty a_k = \sum_{k=2}^\infty \frac{1}{k}$ diverges to infinity, by the Integral Test, $\int_1^\infty \frac{1}{t} dt$ diverges to infinity. This means that there is for large $x$, $\log (x) = \int_1^x \frac{1}{t} dt$ can be made arbitrarily large. This implies that when $x \geq 0$, $\log(x)$ takes on all values in $[0, \infty)$.

Going to the left, for $x \in (0, 1]$, $\log(x) = - \int_x^1 \frac{1}{t} dt$. Let $k \in \N$ and consider $\log(\frac{1}{2^k}) = - \int_\frac{1}{2^k}^1 \frac{1}{t} dt$.

To evaluate $\int_\frac{1}{2^k}^1 \frac{1}{t} dt$, consider the partition $P$ such that $x_i = \frac{1}{2^i}$ for $i \in \N$. Thus $x_0 = 1$, $x_1 = \frac{1}{2}$, $x_2 = \frac{1}{4}$, etc. Because $\frac{1}{t}$ is strictly decreasing, the minimum of $\frac{1}{t}$ occurs at the right endpoint of the interval. Thus the lower integral is greater than or equal to

\begin{align*}
&1(\frac{1}{2}) + 2(\frac{1}{4}) + 4(\frac{1}{8}) \dots \\
=&\frac{1}{2} + \frac{1}{2} + \frac{1}{2} \dots \\
=&\frac{k}{2}
\end{align*}

because there are $k$ intervals. Since $\frac{1}{t}$ is Riemann integrable on $(0, 1]$, $\frac{k}{2}$ is a lower bound for the integral. Thus

\[
- \int_\frac{1}{2^k}^1 \frac{1}{t} dt \leq -\frac{k}{2}
\]

which implies that the integral goes to negative infinity as $k$ goes to infinity. Thus

\[
-\int_0^1 \frac{1}{t} dt = -\infty
\]

which implies that as $x$ approaches zero, $\log(x)$ approaches negative infinity. Thus on $(0, 1]$, $\log(x)$ takes on all values in $(-\infty, 0]$. Putting the two statements together implies that the range of $\log(x)$ is all of $\R$.

\section*{Problem 17}

Define $E: \R \rightarrow \R$ by

\[
E(x) =
\begin{cases}
e^{-1/x} & \text{ if } x > 0 \\
0 & \text{ if } x \leq 0
\end{cases}
\]

\subsection*{Part a}

Prove that $E(x)$ is smooth; that is, $E$ has derivatives of all orders at all points $x$.

For $x < 0$, smoothness is trivial. A quick application of the chain rule shows that on $x > 0$,

\[
E'(x) = \frac{1}{x^2} e^{-\frac{1}{x}}
\]

\begin{theorem}
\label{TheoremE(x)Form}
For $x > 0$, $E^{(n)}(x)$ has the form
\[
(a_{n+1} x^{-(n+1)} + a_{n+2}x^{-(n+2)} + \dots + a_{2n}x^{-2n})e^{-\frac{1}{x}}
\]
for all $n \in \N$.
\begin{proof}
The base case $n=1$ has been established above. Assume that the hypothesis holds for $n-1$. Then
\[
E^{(n-1)}(x) = (a_{n} x^{-n} + a_{n+1}x^{-(n+1)} + \dots + a_{2n-2}x^{-(2n-2)})e^{-\frac{1}{x}}
\]
Using the Product Rule,
\begin{align*}
E^{(n)}(x) &= [(-na_{n} x^{-(n+1)} - (n+1)a_{n+1}x^{-(n+2)} - \dots -(2n+2) a_{2n-2}x^{-(2n-1)}) \\
&+ (a_{n} x^{-(n+2)} + a_{n+1}x^{-(n+3)} + \dots + a_{2n-2}x^{-2n})]e^{-\frac{1}{x}} \\
&= (b_{n+1} x^{-(n+1)} + b_{n+2}x^{-(n+2)} + \dots + b_{2n}x^{-2n})e^{-\frac{1}{x}}
\end{align*}
since $n$ is a constant.
\end{proof}
\end{theorem}

\begin{lemma}
$\lim_{x \rightarrow 0} E(x) = E(0) = 0$. Thus $E(x) \in C^0$.
\begin{proof}
The left limit is trivially zero. On the right, as $x$ approaches zero from the positive direction, $-\frac{1}{x}$ approaches negative infinity, so $e^{-\frac{1}{x}}$ approaches zero. 
\end{proof}
\end{lemma}

\begin{lemma}
$\lim_{x \rightarrow 0^+} \frac{1}{x}e^{-\frac{1}{x}} = 0$.
\begin{proof}
\begin{align*}
\lim_{x \rightarrow 0^+} \frac{e^{-\frac{1}{x}}}{x}
= \lim_{x \rightarrow 0^+} \frac{x^{-1}}{e^{\frac{1}{x}}}
= \lim_{x \rightarrow 0^+} \frac{-x^{-2}}{-x^{-2}e^{\frac{1}{x}}}
= \lim_{x \rightarrow 0^+} e^{-\frac{1}{x}} = 0
\end{align*}
by using l'Hopital's rule on the second expression.
\end{proof}
\end{lemma}

\begin{lemma}
\label{LemmaEn(x)PartsHaveRightLimitZero}
$\lim_{x \rightarrow 0^+} \frac{1}{x^n}e^{-\frac{1}{x}} = 0$ for $n \in \N$.
\begin{proof}
The base case has been established. For the inductive case, assume that $\lim_{x \rightarrow 0^+} \frac{1}{x^{n-1}}e^{-\frac{1}{x}} = 0$. Then

\begin{align*}
\lim_{x \rightarrow 0^+} \frac{e^{-\frac{1}{x}}}{x^n}
= \lim_{x \rightarrow 0^+} \frac{x^{-n}}{e^{\frac{1}{x}}}
= \lim_{x \rightarrow 0^+} \frac{-nx^{-n-1}}{-x^{-2}e^{\frac{1}{x}}}
= n\lim_{x \rightarrow 0^+} \frac{e^{-\frac{1}{x}}}{x^{n-1}} = 0
\end{align*}
by the inductive hypothesis.
\end{proof}
\end{lemma}

\begin{corollary}
\label{CorollaryEn(x)HasRightLimitZero}
$\lim_{x \rightarrow 0^+}E^{(n)}(x) = 0$ when $x > 0$ for all $n \in \N$.
\begin{proof}
By Theorem \ref{TheoremE(x)Form}, $E^{(n)}(x)$ is the sum of various terms of the form $\frac{1}{x^n}e^{-\frac{1}{x}}$, where $n \in N$. By Lemma \ref{LemmaEn(x)PartsHaveRightLimitZero}, each of these terms has right limit zero. Since $\frac{1}{x^n}e^{-\frac{1}{x}}$ is the sum of a finite number of these terms, it has right limit zero.
\end{proof}
\end{corollary}

\begin{theorem}
$E^{(n)}(0)$ exists and it equals zero for all $n \in \N$. $E^{(n)}(x)$ is continuous at $x=0$ for all $n \in \N$, thus making $E(x)$ smooth.
\begin{proof}
For $n \in N$, we need to evaluate
\[
\lim_{x \rightarrow 0} \frac{E^{(n-1)}(x)-0}{x-0} = 
\lim_{x \rightarrow 0} \frac{E^{(n-1)}(x)}{x}
\]

The left limit is zero, since $E^{(n-1)}(x) = 0$ for $x \leq 0$. For the right limit, by Theorem \ref{TheoremE(x)Form}, $\frac{1}{x}E^{(n-1)}(x)$ has the form
\[
(a_{n+2} x^{-(n+2)} + a_{n+3}x^{-(n+3)} + \dots + a_{2n+1}x^{-2n+1})e^{-\frac{1}{x}}
\]

By repeated application of Lemma \ref{LemmaEn(x)PartsHaveRightLimitZero}, we see that this has right limit zero. Thus
\[
E^{(n)}(0) = \lim_{x \rightarrow 0} \frac{E^{(n-1)}(x)}{x} = 0
\]
Because $E^{(n)}(x) = 0$ on $x \leq 0$, $E^{(n)}(0) = 0$, and $E^{(n)}(x)$ is continuous at $x = 0$ for all $n \in \N$. Combined with smoothness everywhere else, this implies that $E(x)$ is smooth everywhere.
\end{proof}
\end{theorem}

\subsection*{Part b}

Is $E(x)$ analytic?

No. A function $f$ defined on an open interval $(a, b)$ is analytic at $x \in (a, b)$ if it equals its a power series in a neighborhood of $x$. More specifically, for every $x$ there exists a $\delta > 0$ such that $|h| < \delta$ implies that

\[
f(x+h) = \sum_{r=0}^\infty a_r h^r
\]

where $a_r = \frac{f^{(r)}(x)}{r!}$.

For $E(x)$ at $x=0$, $E^{(n)}(0) = 0$ for all whole numbers $n$, as established in Part a. Thus $a_r = 0$ for all whole numbers $r$, and the power series is just $0$. However, for $h > 0$, $f(h) \neq 0$ since $e^{-\frac{1}{x}}$ is a strictly positive function. Thus $E(x)$ is not analytic.

\section*{Problem 29}

Prove that the interval $[a, b]$ is not a zero set.

\subsection*{Part a}

Explain why the following observation is not a solution to the problem: "Every open interval that contains $[a, b]$ has length $> b-a$."

This 'solution' does not consider the possibility that there is a union of open sets that cover $[a, b]$ such that their sum of their lengths can be made arbitrarily small.

\subsection*{Part b}

Instead, suppose there is a "bad" covering of $[a, b]$ by open intervals $\{I_i\}$ whose total length is $< b-a$, and justify the following steps in the proof by contradiction.

I will define a good covering as a covering of $[a, b]$ by open intervals $\{J\}$ such that the total length of the intervals in $\{J\}$ is greater than or equal to $b-a$.

\subsubsection*{i}

It is enough to deal with finite bad coverings.

Let $\{I\}$ be an infinite bad covering of $[a, b]$. Because $\{I\}$ is an open cover of compact $[a, b]$, it reduces to a finite subcovering $\{I_i\}$. Thus, either $\{I\}$ reduces to a finite bad covering, or it reduces to a good covering. If $\{I\}$ reduces to a good covering $\{J_i\}$, then  $\{J_i\} \subset \{I\}$ and the sum of the intervals in $\{J_i\}$ being $\geq b-a$ implies that the sum of the intervals in $\{I\}$ is $\geq b-a$. Thus $\{I\}$ is an infinite good covering, which contradicts the assumption that $\{I\}$ is a bad covering.

Thus, if $\{I\}$ is an infinite bad covering, it reduces to a finite bad covering. Contrapositively, if there are no finite bad coverings, then there are no infinite bad coverings, and the theorem is proven.

\subsubsection*{ii}

Let $\B = \{I_1, \dots I_n\}$ be a bad covering such that $n$ is minimal among all bad coverings.

There is at least one finite bad covering, by assumption. $n=1$ is a lower bound for the size of bad coverings. Then because $\R$ is complete, there exists a greatest lower bound for the sizes of the bad coverings, denoted $c$.

The must be a finite bad covering $\{C\}$ such that the size of $|\{C\}| = c$. Suppose not. Then all bad coverings have size $> c$, and size the sizes of the bad coverings must be integers, all bad coverings have size $\geq c+1$. This contradicts the assumption that $c$ is a greatest lower bound. This bad covering $\{C\}$ is the bad covering with minimal $n$ among all bad coverings.

\subsubsection*{iii}

Show that no bad covering has $n=1$ so we have $n \geq 2$.

This follows from the observation in Part a.

\subsubsection*{iv}

Show that it is no loss of generality to assume $a \in I_1$ and $I_1 \cap I_2 \neq \emptyset$.

There exists at least one interval such that $a \in I_j$, and we are free to denote that interval $I_1$.

There must exist an interval that intersects $I_1$. Suppose not. Let $d_1$ be the right endpoint of $I_1$, and let $c_2, c_3 \dots c_n$ be the left endpoints of the other intervals in the bad covering, and let $c = \min\{c_1 \dots c_n\}$. Then $\frac{c-d}{2}$ is not covered by the bad covering, contradicting the assumption that $\{I\}$ is a covering. Thus, there exists an interval in $\{I\}$ that intersects $I_1$. Denote it $I_2$. By construction, $I_1 \cap I_2$ is nonempty.

\subsubsection*{v}

Show that $I = I_1 \cup I_2$ is an open interval and $|I| < |I_1| + |I_2|$.

If $I_1 \subset I_2$ or $I_2 \subset I_1$, $I_1 \cup I_2$ is trivially an open interval. Otherwise, $I_1 \cup I_2$ is the open because it is the union of open sets, connected because it is the union of two connected sets with a common point, and bounded because it is the finite union of bounded sets. Therefore $I_1 \cup I_2$ is a open, connected, and bounded subset of $\R$, and by the theorems shown in Chapter 2 Problem 31, open, connected, and bounded subsets of $\R$ are open intervals.

\begin{lemma}
Let $C, D \subset \R$ be (bounded) intervals that intersect, and let $E = C + D$. Then $|E| < |C| + |D|$.

\begin{proof}
If $C$ is a subset of $D$ or vice versa, the proof is trivial. Without loss of generality, let the left endpoint of $C$ be less than the left endpoint of $D$. Denote $c$ as the right endpoint of $C$, and $d$ the left endpoint of $D$. $d < c$, otherwise the two intervals do not intersect. Letting $\epsilon = c - d > 0$, the total length of $E$ is $|C| + |D| - \epsilon$, which is strictly less than $|C| + |D|$.
\end{proof}
\end{lemma}

By using the above Lemma, we see that $|I| < |I_1| + |I_2|$.

\subsubsection*{vi}

Show that $\B' = \{I, I_3, \dots I_n\}$ is a bad covering of $[a, b]$ with fewer intervals, contradicting the minimality of $n$.

Let $x \in [a, b]$. Since $\B$ is a covering of $[a, b]$, there exists $i \in 1, 2 \dots n$ such that $x \in I_i$. If $i \geq 3$, then because $I_i \in \B'$, $x$ is also covered by $\B'$. If $i = 1, 2$, then $x \in I = I_1 \cup I_2$, so $x$ is still covered by $\B'$. $\B'$ is a covering by open intervals, because $I$ is an open interval. $\B'$ is a bad covering. $|I| < |I_1| + |I_2|$ implies that $|I| + \sum_{j=3}^n I_j < \sum_{i=1}^n I_i < b-a$, implying that the total length of $\B'$ is less than the total length of $\B$. Thus $\B'$ is a bad covering with fewer intervals than $\B$, contradicting the assumption that $\B$ is the minimal bad covering. Thus, there are no bad coverings of $[a, b]$, coverings of $[a, b]$ can not have arbitrarily small length, and $[a, b]$ is not a zero set.

\section*{Problem 34}

Assume that $\psi: [a, b] \rightarrow \R$ is continuously differentiable. A critical point of $\psi$ is an $x$ such that $\psi'(x) = 0$. A critical value is a number $y$ such that for at least one critical point $x$ we have $y = \psi(x)$.

\subsection*{Part a}

Prove that the set of critical values is a zero set. (This is the Morse-Sard Theorem in dimension one.)

I will first introduce some notation. Let $f: [a, b] \rightarrow \R$ be continuous. I will define a \textbf{zero of type 1} of $f$ to be a zero of $f$ such that $f$ is uniformly zero in an open neighborhood of the root. In other words, if $f(x) = 0$, then there exists $\epsilon > 0$ such that for all $y$ such that $|x-y| < \epsilon$, $f(y) = 0$. I will denote a \textbf{zero point of type 2} of $f$ as all other zeros of $f$. It's clear that the disjoint union of zeros of types 1 and 2 make up all zeros of $f$.

Let $\psi$ be continuously differentiable. We will characterize the critical values of $\psi$ based on the zeros of type 1 and 2 of $\psi'$. If $\psi(x) = y$ is a critical value and $\psi'(x)$ is a zero of type 1, we say that $y$ is a \textbf{critical value of type 1} of $f$. Similarly, if $\psi(x) = y$ is a critical value and $\psi'(x)$ is a zero of type 2, we say that $y$ is a \textbf{critical value of type 2} of $f$. Since the zeros of type 1 and 2 partition the set of zeros of $\psi'$, the critical values of type 1 and 2 partition the set of critical values of $\psi$.

I have no idea if this characterization is standard, but that's what I've come up with.

The immediate characterization for zeros of type 2 is stated below.

\begin{lemma}
\label{ZeroType2ImpliesNonzeroPoint}
Let $f$ be continuous, and let $x$ be a zero of type 2 of $f$. Then for all $\epsilon > 0$, there exists $y \in (x-\epsilon, x + \epsilon)$ such that $f(y) \neq 0$.
\begin{proof}
If this is not true, then $x$ is a zero of type 1.
\end{proof} 
\end{lemma}

To begin with zero points of type 2, we next state a lemma on non-zero points of continuous functions implying an interval with no zeros. This can be thought of as non-zero points of continuous functions creating 'exclusion zones' with a delta-radius that contain no zeros.

\begin{lemma}
\label{ExclusionZoneDelta}
Let $f: [a, b] \rightarrow \R$ be continuous, and let $x \in [a, b]$ be a point such that $f(x) \neq 0$. Then there exists a $\delta > 0$ such that $f$ has no zeros in $(x-\delta, x+\delta)$.
\begin{proof}
Because $f$ is uniformly continuous, there exists a $\delta > 0$ such that for all $y$ such that $|x-y| < \delta$, $|f(x) - f(y)| < |f(x)|$. This implies that $y$ is not a zero of $f$.
\end{proof}
\end{lemma}

\begin{lemma}
\label{LemmaType2NotCluster}
Let $f: [a, b] \rightarrow \R$ be continuous, and let $x \in [a, b]$ such that $f(x) \neq 0$. Then $f$ is not a clustering point of zeros. In other words, we can reasonably speak of the nearest zero of $f$ greater than $x$, and the nearest zero of $f$ less than $x$.
\begin{proof}
Suppose not. Then there exists a sequence $(x_n) \rightarrow x$ of zeros of $f$. $\lim_{n \rightarrow \infty} f(x_n) = 0$, but $f(x) \neq 0$, violating the continuity of $f$.
\end{proof}
\end{lemma}

We now introduce some useful terminology (that I have no idea whether is standard, but I am going to use it). Let $f: [a, b] \rightarrow \R$ be continuous, and let $x$ be a point such that $f(x) \neq 0$. The \textbf{covering interval of x} is the open interval between the nearest zero of $f$ to the left of $x$, and the nearest zero of $f$ to the right of $x$. This interval covers $x$. By Lemma \ref{LemmaType2NotCluster}, this is a well-defined construction.

I will denote the interval as $C$. If there are no zeros of $f$ to the left of $x$, then the left endpoint of $C$ is $a$, and if there are no zeros to the right of $x$, then the right endpoint of $C$ is $b$.

\begin{lemma}
Let $f: [a, b] \rightarrow \R$ be continuous. Then the covering intervals of $f$ are disjoint.
\begin{proof}
Any two covering intervals are separated by at least one zero of $f$.
\end{proof}
\end{lemma}

We now show that the number of covering intervals is closely related to the number of zeros of type 2.

\begin{lemma}
Let $x$ be a zero of type 2. Then there is a covering interval such that $x$ is the endpoint. 
\begin{proof}
Suppose not. Then $x$ is not the nearest zero of type 2 to any nonzero point of $f$. This means that $x$ is a clustering point of zeros of type 2, which contradicts Lemma \ref{LemmaType2NotCluster}.
\end{proof}
\end{lemma}

\begin{corollary}
\label{CorollaryZeroType2Cardinality}
The set of zeros of type 2 is of equal or lesser cardinality to the set of covering intervals.
\begin{proof}
Each zero of type 2 belongs to at least one covering interval.
\end{proof}
\end{corollary}

The main results for zeros of type 2 follows.

\begin{corollary}
Let $f$ be continuous on $[a, b]$. Then $f$ has at most countable zeros of type 2.
\begin{proof}
Let $Q$ be the set of covering intervals for $f$. Because $Q$ is the disjoint union of intervals, $Q$ is countable. By Lemma \ref{CorollaryZeroType2Cardinality}, the zeros of type 2 of $f$ are countable.
\end{proof}
\end{corollary}

\begin{corollary}
Let $f$ be continuously differentiable on $[a, b]$. Then $f$ has at most countable critical values of type 2.
\begin{proof}
$f'$ is continuous, implying that $f$ has at most countable zeros of type 2. Each zero of type 2 of $f'$ maps to at most one critical value of type 2 of $f$.
\end{proof}
\end{corollary}

We now turn to critical points of type 1. We first state a useful characterization of zeros of type 1.

\begin{lemma}
\label{LemmaZerosType1}
Let $f: [a, b] \rightarrow \R$ be continuous, and let $Z$ be the set of zeros of type 1. Then $Z$ is the  disjoint union of countable open intervals, with perhaps one or two half-open intervals at the endpoints $a$ and $b$.
\begin{proof}
The definition of zeros of type 1 implies that $Z$ is an open set in $[a, b]$. By the Inheritance Principle, there exists a set $W \subset \R$ that is open in $\R$ such that $W \cap \R = Z$. By Problem 31 in Chapter 2, an open set in $\R$ can be expressed as the disjoint union of countably many open intervals. Taking $W \cap Z$, open intervals that do not contain the endpoints $a$ and $b$ are still open in $Z$, while the half-intervals that have their closed end at $a$ and $b$ become open in $[a, b]$.
\end{proof}
\end{lemma}

We next state a lemma on critical points of type 1.

\begin{lemma}
\label{LemmaCriticalValuesType1}
Let $x$ be a critical point of type 1 for $\psi'(x)$. Then on the neighborhood where $\psi'(x) = 0$, there is only one critical value. Specifically, if $\psi'(x) = 0$ on an interval $(c, d) \subset [a, b]$, then $\psi(c)$ is the only critical value on that interval.
\begin{proof}
By the Fundamental Theorem of Calculus, for $x \in [c, d]$, $\psi(x) = \psi(c) + \int_c^x \psi'(x) dx = \psi(c)$ since $\psi'(x) = 0$ on the interval. 
\end{proof}
\end{lemma}

We now want to prove the main theorem for critical values corresponding to critical points of type 1.

\begin{theorem}
If $f: [a, b] \rightarrow \R$ is continuously differentiable, then $f$ has countably many critical values of type 1.
\begin{proof}
$f'$ is continuous by assumption. By Lemma \ref{LemmaZerosType1}, $Z$, the set of zeros of type 1 of $f'$, consists of countable disjoint open intervals, with perhaps one or two half-open intervals at $a$ and $b$. By Lemma \ref{LemmaCriticalValuesType1}, each (half)-open interval in $Z$ corresponds to one critical value in $f$. Thus $f$ has at most countably many critical values of type 1.
\end{proof}
\end{theorem}

\begin{theorem}
The critical values of $f$ form a zero set.
\begin{proof}
The union of countable sets is a countable set, which is a zero set.
\end{proof}
\end{theorem}

\subsection*{Part b}

Generalize this result to continuous functions on $\R \rightarrow \R$.

The result immediately generalizes. Divide $\R$ into countably many intervals of length $1$. By Part a, there are countably many critical values of $f$ on each of these intervals, and the countable union of countable sets is a zero set. Thus the set of critical values of a continuously differentiable function $f: \R \rightarrow \R$ is a zero set. 

\section*{Problem 36}

We say that $f: (a, b) \rightarrow \R$ has a \textbf{jump discontinuity} (or a discontinuity of the \textbf{first kind}) at $c \in (a, b)$ if

\[
f(c^-) = \lim_{x \rightarrow c^-} f(x) \text{ and } f(c^+) = \lim_{x \rightarrow c^+} f(x)
\]

exist, but are either unequal or are unequal to $f(c)$. An \textbf{oscillating discontinuity} (or a discontinuity of the \textbf{second kind} is any nonjump discontinuity).

\subsection*{Part a}

Show that $f: \R \rightarrow \R$ has at most countably many jump discontinuities.

I will first start with showing that $f: [0, 1] \rightarrow \R$ has countably many jump discontinuities. Let $M(c)$ be defined as

\[
M(c) =
\begin{cases}
\max\{ |f(c^-) - f(c)|, |f(c^+) - f(c)|\} & \text{ if } f \text{ has a jump discontinuity at } c \\
0 & \text{ else}
\end{cases}
\]

Let $A_n$ be the set of points in $\R$ such that $M(c) > \frac{1}{n}$. It's clear that the set of jump discontinuities of $f$ is $A = \cup_{n=1}^\infty A_n$.

For all $c \in A_n$, because $c$ is a jump discontinuity, there exists $\delta_-, \delta_+ > 0$ such that $x \in (c-\delta_-, c)$ implies that $|f(x) - f(c^-)| < \frac{1}{2n}$, with a similar result for $\delta_+$. Let $\delta = \min (\delta_-, \delta_+)$. Note that $\delta$ depends on $c$.

I claim that $(c - \delta, c + \delta) - \{c\} \subset A_n^C$. Let $x \in (c - \delta, c + \delta) - \{c\}$. If $f$ is continuous at $x$, or if $x$ is an oscillating discontinuity of $x$, the result is trivial.

If $x$ is a jump discontinuity of $x$, then the left and right limits of $f$ exist at $x$. Let $\gamma = \min\{|x-c|, |x - (c-\delta)|, |x-(c+\delta)| \}$. If $p_n$ is an arbitrary sequence such that $p_n \rightarrow x$, then eventually the tail of $p_n$ will lie entirely in $(x-\gamma, x+\gamma) \subset (c - \delta, c + \delta) - \{c\}$. Thus

\[
\diam f((x - \gamma, x + \gamma)) < \frac{1}{n}
\]

implies that $x$ is not in $A_n$. Thus $A_n$ is a zero set, implying that $A$ is a zero set. Thus $f:[0, 1] \rightarrow \R$ has at most countably many jump discontinuities. Repeating this argument for all $\Z$ shows that $f:\R \rightarrow \R$ has at most countably many jump discontinuities.

\subsection*{Part b}

What about the function
\[
f(x) = 
\begin{cases}
\sin \frac{1}{x} & \text{ if } x > 0 \\
0 & \text{ if } x \leq 0
\end{cases}
\]

$f$ is continuous when $x \neq 0$, since it on the left it is a constant function, and on the right it is the composition of continuous functions. $f$ has an oscillating discontinuity at $x=0$. Consider the sequences $p_n = 1/ (\frac{\pi}{2} + 2n)$ and $q_n = 1/ (\frac{3\pi}{2} + 2n)$. Both of these sequences converge to zero, but $f(p_n) = 1$ and $f(q_n) = -1$ for all $n \in \N$. Thus the right limit of $f$ does not exist at $0$, and so $x=0$ is an oscillating discontinuity.

\subsection*{Part c}

What about the characteristic function of the rationals?

$f$ is oscillating discontinuous everywhere. For at any point $x$, there are a convergent sequence of rationals, and a convergent series of irrationals approaching $x$, implying that the limit of $f$ at $x$ does not exist.

\section*{Problem 39}

Consider the characteristic functions $f(x)$ and $g(x)$ of the intervals $[1, 4]$ and $[2, 5]$. The derivatives $f'$ and $g'$ exist almost everywhere. The integration by parts formula says that

\[
\int_0^3 f(x)g'(x) dx = f(3)g(3) - f(0)g(0) - \int_0^3 f'(x)g(x)dx
\]

But both integrals are zero, while $f(3)g(3) - f(0)g(0) = 1$. Where is the error?

The textbook integration by parts formula assumes that $f$ and $g$ are continuous on $[a, b]$. In this case, the interval in question is $[0, 3]$. However, $f$ is discontinuous at $x=1$, and $g$ is discontinuous at $x = 2$.

Specifically, the Leibniz formula states that if $f, g$ are continuous at $x$, then $(fg)'(x) = (f'g)(x) + (fg')(x)$. If $f, g$ are continuous on the interval in question then the Leibniz formula says that $f'g + fg'$ is an antiderivative of $fg$, Then by the Antiderivative Theorem, the indefinite integral of $fg' + f'g$ differs from the antiderivative by a constant.

However, in this case, $fg'$ is zero on $[0, 3]$, except at $x=2$, where it is undefined. A similar thing holds for $f'g$ at $x=1$. Since a potential antiderivative of $fg'$ has to have its derivative equal $fg'$ everywhere, not just almost everywhere, $fg'$ and $f'g$ do not have antiderivatives.

In fact, because $fg'$ is zero almost everywhere, $\int_0^3 f(x)g'(x) dx = 0$.

\end{document}